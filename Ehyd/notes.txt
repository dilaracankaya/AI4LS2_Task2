
########################################################################################################################
# Yer alt? su seviyesi – 3792 *
# Yer alt? s?cakl?k – 1562 *
# Messstellen – 3792
#
# Ya?mur – 863 *
# Kar – 631 *
# Messstellen – 907
#
# Fahig iletkenlik – 90 *
# Schüt debi – 93 *
# Temp – 93 *
# messstellen – 93
#
# w-tages nehir seviyesi – 759 (iki csv'de data yok ama o yüzden 757 gelmeli) *
# wt-monat temp – 322 *
# schweb sediment – 35 *
# q tages debi - 625 *
# messstellen - 792
########################################################################################################################


Sources:
- Quellleitfahigkeit -  sularda ölçülen elektriksel iletkenlik (konduktans) bilgilerini içermektedir. µS/cm
# Failed to create DataFrame: Quellleitfähigkeit-Tagesmittel-396796.csv, error: could not convert string to float: 'rekonstruiert aus Version 3->'

- Quellshüttung - su debisi (ak?? h?z?) ölçümlerini içermektedir
# Failed to create DataFrame: Quellschüttung-Tagesmittel-396093.csv, error: could not convert string to float: 'F'

- Temperature
# Failed to create DataFrame: Quellwassertemperatur-Tagesmittel-396010.csv, error: could not convert string to float: 'K'


Surface Water:
- W – nehir su seviyesi
# Failed to create DataFrame: W-Tagesmittel-204388.csv, error: time data 'Invalid' does not match format '%d.%m.%Y %H:%M'
# Failed to create DataFrame: W-Tagesmittel-211078.csv, error: time data 'Invalid' does not match format '%d.%m.%Y %H:%M'
# data yok, k?zlardan biri de indirdikleri dosyalar? bir kontrol etsin

- Wt – su s?cakl???


- Schweb – sediment (Ask?da (çözünmü?) kat? madde miktar?, nehirdeki su ak???yla ta??nan küçük parçac?klar?n
(kum, kil, organik maddeler vb.) miktar?n? gösterir. Bu ölçümler, nehrin ta??d??? sediment miktar?n? anlamak ve nehir ekosisteminin durumunu de?erlendirmek için önemlidir.)


- Q – debi


- Gesch (az olan) - Yatak yükü, ask?da kat? maddeden farkl? olarak daha büyük parçac?klar?n hareketini temsil eder. Nehirlerin dibinde ta??n?r, akarsu ittire ittire ta??n?r gibi.



# in case
#########################################################################################################################3

# Tarihler aras?nda filtreleme yap
df_379313["Date"] = pd.to_datetime(df_379313["Date"])
# df_379313 = df_379313[(df_379313["Date"] >= "1980-12-01") & (df_379313["Date"] <= "2021-12-01")]



def set_day_to_first(df):
    df['Date'] = df['Date'].apply(lambda x: x.replace(day=1))
    return df

for df_name in df_379313_rain_list:  # bu, a?a?daki fonksiyon ile birle?tirilebilir
    df = globals().get(df_name)
    df = set_day_to_first(df)
    df = df[(df["Date"] >= "1980-12-01") & (df["Date"] <= "2021-12-01")]
    print(df.head())


def fill_missing_dates_nan(df, start_date="1980-12-01", end_date="2021-12-01"):
    # T?m tarih aral???n? olu?tur
    all_dates = pd.date_range(start=start_date, end=end_date, freq='MS')

    # Veri ?er?evesini t?m tarihlerle yeniden indeksleyin ve eksik de?erleri NaN yap?n
    df = df.set_index('Date').reindex(all_dates).reset_index()
    df.columns = ['Date'] + list(df.columns[1:])  # 'Date' kolonunu tekrar adland?r?n

    return df


for df_name in df_379313_rain_list:
    df = globals().get(df_name)
    if df is not None:
        df = fill_missing_dates_nan(df)
        globals()[df_name] = df
        print(df.head())




merge_list = [df_379313]
for df_name in df_379313_rain_list:
    df = globals().get(df_name)
    merge_list.append(df)


# merge
merged_df = merge_list[0]

# Listeyi gezerek t?m DataFrame'leri birle?tiriyoruz
for i, df in enumerate(merge_list[1:], start=1):
    # Her yeni DataFrame'den gelen kolonlara _i ekliyoruz
    suffixes = (None, f'_df{i}')
    merged_df = pd.merge(merged_df, df, on='Date', how='inner', suffixes=suffixes)



merged_df.columns = ['Date', 'df_379313', 'df_100933', 'df_100925', 'df_100404', 'df_100941', 'df_100412', 'df_100503', 'df_100883', 'df_115352', 'df_100370', 'df_100529']
merged_df.head()

# Lagged merged df

# Lag de?erini eklemek istedi?iniz kolonlar
cols_to_lag = ['df_100933', 'df_100925', 'df_100404', 'df_100941', 'df_100412', 'df_100503', 'df_100883', 'df_115352', 'df_100370', 'df_100529']

# Yeni bir veri ?er?evesi olu?tur
lagged_merged_df = merged_df.copy()

# Lag kolonlar?n? olu?tur
for col in cols_to_lag:
    lagged_merged_df[f'{col}_lag1'] = merged_df[col].shift(1)
    lagged_merged_df[f'{col}_lag2'] = merged_df[col].shift(2)

# Kolonlar? s?ralamak i?in ?nce lag1'ler ve sonra lag2'ler
# ?nce t?m kolonlar? listeleyin
lag1_cols = [f'{col}_lag1' for col in cols_to_lag]
lag2_cols = [f'{col}_lag2' for col in cols_to_lag]
merged_df_cols = merged_df.columns.to_list()

# Kolon s?ralamas?
new_order = merged_df_cols  + lag1_cols + lag2_cols

# Veri ?er?evesini yeni kolon s?ralamas?na g?re d?zenle
lagged_merged_df = lagged_merged_df[new_order]

# Sonu?lar? g?r?nt?leme
print(lagged_merged_df.head())

len(lagged_merged_df.columns)

# Heatmap
data = lagged_merged_df.iloc[:, -31:]
plt.figure(figsize=(20, 20))
dataplot = sns.heatmap(data.corr(), cmap="YlGnBu", annot=True, annot_kws={'size': 8})

# Grafi?i kaydetmek
plt.savefig("Ehyd/datasets_ehyd/heatmap_80s_with_nan_n_lags.png", bbox_inches='tight')

# Grafi?i g?stermek
plt.show()


merged_df.shape  # (493, 12)
merged_df.isnull().sum()
# merged_df.isnull().sum()
# Out[85]:
# Date           0
# df_379313     53
# df_100933    128
# df_100925    109
# df_100404      0
# df_100941    127
# df_100412      0
# df_100503      0
# df_100883    109
# df_115352    361
# df_100370      0
# df_100529      0
# dtype: int64